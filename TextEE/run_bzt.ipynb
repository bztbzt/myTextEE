{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from argparse import Namespace\n",
    "from models import *\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertConfig, RobertaConfig, XLMRobertaConfig, BertModel, RobertaModel, XLMRobertaModel,RobertaTokenizer\n",
    "from collections import namedtuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ED\";\n",
    "dataset = \"phee\";\n",
    "split = 1;\n",
    "model_type = \"CRFTagging\";\n",
    "pretrained_model_name = \"roberta-base\";\n",
    "pretrained_model_alias = {\n",
    "    \"roberta-base\": \"roberta-base\", \n",
    "};\n",
    "config_dict =  {\n",
    "        #// general config\n",
    "        \"task\": task, \n",
    "        \"dataset\": dataset,\n",
    "        \"model_type\": model_type, \n",
    "        \"gpu_device\": 0, \n",
    "        \"seed\": 0, \n",
    "        \"cache_dir\": \"./cache\", \n",
    "        \"output_dir\": \"./outputs/%s_%s_%s_split%s_%s\" % (model_type, task, dataset, split, pretrained_model_alias[pretrained_model_name]), \n",
    "        \"train_file\": \"../data/preprocessing/%s/split%s/train.json\" % (dataset, split),\n",
    "        \"dev_file\": \"../data/preprocessing/%s/split%s/dev.json\" % (dataset, split),\n",
    "        \"test_file\": \"../data/preprocessing/%s/split%s/test.json\" % (dataset, split),\n",
    "        \n",
    "        \n",
    "        #// model config\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"base_model_dropout\": 0.2,\n",
    "        \"use_crf\": True,\n",
    "        \"type_feature_num\": 100, \n",
    "        \"linear_hidden_num\": 150,\n",
    "        \"linear_dropout\": 0.2,\n",
    "        \"linear_bias\": True, \n",
    "        \"linear_activation\": \"relu\",\n",
    "        \"multi_piece_strategy\": \"average\", \n",
    "        \"max_length\": 200, \n",
    "        \n",
    "        # // train config\n",
    "        \"max_epoch\": 30,\n",
    "        \"warmup_epoch\": 5,\n",
    "        \"accumulate_step\": 1,\n",
    "        \"train_batch_size\": 6,\n",
    "        \"eval_batch_size\": 12,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"base_model_learning_rate\": 1e-05,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"base_model_weight_decay\": 1e-05,\n",
    "        \"grad_clipping\": 5.0,\n",
    "    }\n",
    "config = Namespace(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trainer\n",
    "VALID_TASKS = [\"E2E\", \"ED\", \"EAE\", \"EARL\"]\n",
    "\n",
    "TRAINER_MAP = {\n",
    "\n",
    "    (\"CRFTagging\", \"ED\"): CRFTaggingEDTrainer, \n",
    "    (\"CRFTagging\", \"EAE\"): CRFTaggingEAETrainer\n",
    "}\n",
    "trainer_class = TRAINER_MAP[(config.model_type, config.task)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2897 ED instances (2 trigger types) from ../data/preprocessing/phee/split1/train.json\n",
      "Loaded 965 ED instances (2 trigger types) from ../data/preprocessing/phee/split1/dev.json\n",
      "Loaded 965 ED instances (2 trigger types) from ../data/preprocessing/phee/split1/test.json\n",
      "There are 2 trigger types in total\n"
     ]
    }
   ],
   "source": [
    "def load_ED_data(file, add_extra_info_fn, config):\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as fp:\n",
    "        lines = fp.readlines()\n",
    "    data = [json.loads(line) for line in lines]\n",
    "    \n",
    "    instances = []\n",
    "    for dt in data:\n",
    "\n",
    "        event_mentions = dt['event_mentions']\n",
    "        event_mentions.sort(key=lambda x: x['trigger']['start'])\n",
    "\n",
    "        triggers = []\n",
    "        for i, event_mention in enumerate(event_mentions):\n",
    "            # trigger = (start index, end index, event type, text span)\n",
    "            trigger = (event_mention['trigger']['start'], \n",
    "                       event_mention['trigger']['end'], \n",
    "                       event_mention['event_type'], \n",
    "                       event_mention['trigger']['text'])\n",
    "\n",
    "            triggers.append(trigger)\n",
    "\n",
    "        triggers.sort(key=lambda x: (x[0], x[1]))\n",
    "        \n",
    "        instance = {\"doc_id\": dt[\"doc_id\"], \n",
    "                    \"wnd_id\": dt[\"wnd_id\"], \n",
    "                    \"tokens\": dt[\"tokens\"], \n",
    "                    \"text\": dt[\"text\"], \n",
    "                    \"triggers\": triggers,\n",
    "                   }\n",
    "\n",
    "        instances.append(instance)\n",
    "\n",
    "    trigger_type_set = set()\n",
    "    for instance in instances:\n",
    "        for trigger in instance['triggers']:\n",
    "            trigger_type_set.add(trigger[2])\n",
    "\n",
    "    type_set = {\"trigger\": trigger_type_set}\n",
    "    \n",
    "    # approach-specific preprocessing\n",
    "    new_instances = add_extra_info_fn(instances, data, config)\n",
    "    assert len(new_instances) == len(instances)\n",
    "    \n",
    "    print('Loaded {} ED instances ({} trigger types) from {}'.format(\n",
    "        len(new_instances), len(trigger_type_set), file))\n",
    "    \n",
    "    return new_instances, type_set\n",
    "train_data, train_type_set = load_ED_data(config.train_file, trainer_class.add_extra_info_fn, config)\n",
    "dev_data, dev_type_set = load_ED_data(config.dev_file, trainer_class.add_extra_info_fn, config)\n",
    "test_data, test_type_set = load_ED_data(config.test_file, trainer_class.add_extra_info_fn, config)\n",
    "type_set = {\"trigger\": train_type_set[\"trigger\"] | dev_type_set[\"trigger\"] | test_type_set[\"trigger\"]}\n",
    "print(\"There are {} trigger types in total\".format(len(type_set[\"trigger\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /roberta-base/resolve/main/vocab.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001CC82CD5E10>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/vocab.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "tokenizer = RobertaTokenizer.from_pretrained(config.pretrained_model_name, cache_dir=config.cache_dir, do_lower_case=False, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "        \n",
    "        print(\"Removing overlapping triggers and over-length examples\")\n",
    "        \n",
    "        # greedily remove overlapping triggers\n",
    "        n_total = 0\n",
    "        new_data = []\n",
    "        for dt in data:\n",
    "            \n",
    "            n_total += 1\n",
    "            \n",
    "            if len(dt[\"tokens\"]) > config.max_length:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            no_overlap_flag = np.ones((len(dt[\"tokens\"]), ), dtype=bool)\n",
    "            new_triggers = []\n",
    "            for trigger in dt[\"triggers\"]:\n",
    "                start, end = trigger[0], trigger[1]\n",
    "                if np.all(no_overlap_flag[start:end]):\n",
    "                    new_triggers.append(trigger)\n",
    "                    no_overlap_flag[start:end] = False\n",
    "                            \n",
    "            pieces = [tokenizer.tokenize(t, is_split_into_words=True) for t in dt[\"tokens\"]]\n",
    "            token_lens = [len(p) for p in pieces] \n",
    "\n",
    "            new_dt = {\"doc_id\": dt[\"doc_id\"], \n",
    "                      \"wnd_id\": dt[\"wnd_id\"], \n",
    "                      \"tokens\": dt[\"tokens\"], \n",
    "                      \"pieces\": [p for w in pieces for p in w], \n",
    "                      \"token_lens\": token_lens, \n",
    "                      \"token_num\": len(dt[\"tokens\"]), \n",
    "                      \"text\": dt[\"text\"], \n",
    "                      \"triggers\": new_triggers\n",
    "                     }\n",
    "            \n",
    "            new_data.append(new_dt)\n",
    "                \n",
    "        print(f\"There are {len(new_data)}/{n_total} ED instances after removing overlapping triggers and over-length examples\")\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing overlapping triggers and over-length examples\n",
      "There are 2897/2897 ED instances after removing overlapping triggers and over-length examples\n",
      "Removing overlapping triggers and over-length examples\n",
      "There are 965/965 ED instances after removing overlapping triggers and over-length examples\n"
     ]
    }
   ],
   "source": [
    "internal_train_data = process_data(train_data)\n",
    "internal_dev_data = process_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '10030778_1',\n",
       " 'wnd_id': '10030778_1_1',\n",
       " 'tokens': ['Intravenous', 'azithromycin', '-', 'induced', 'ototoxicity', '.'],\n",
       " 'pieces': ['ĠInt',\n",
       "  'ra',\n",
       "  'ven',\n",
       "  'ous',\n",
       "  'Ġaz',\n",
       "  'ith',\n",
       "  'romy',\n",
       "  'cin',\n",
       "  'Ġ-',\n",
       "  'Ġinduced',\n",
       "  'Ġot',\n",
       "  'ot',\n",
       "  'oxicity',\n",
       "  'Ġ.'],\n",
       " 'token_lens': [4, 4, 1, 1, 3, 1],\n",
       " 'token_num': 6,\n",
       " 'text': 'Intravenous azithromycin - induced ototoxicity .',\n",
       " 'triggers': [(3, 4, 'Adverse_event', 'induced')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trigger': {'O': 0, 'B-Adverse_event': 1, 'I-Adverse_event': 2, 'B-Potential_therapeutic_event': 3, 'I-Potential_therapeutic_event': 4}}\n"
     ]
    }
   ],
   "source": [
    "prefix = ['B', 'I']\n",
    "trigger_label_stoi = {'O': 0}\n",
    "for t in sorted(type_set[\"trigger\"]):\n",
    "    for p in prefix:\n",
    "        trigger_label_stoi['{}-{}'.format(p, t)] = len(trigger_label_stoi)\n",
    "\n",
    "label_stoi = {\"trigger\": trigger_label_stoi}\n",
    "print(label_stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "EDBatch_fields = ['batch_doc_id', 'batch_wnd_id', 'batch_tokens', 'batch_pieces', 'batch_token_lens', 'batch_token_num', 'batch_text', 'batch_triggers']\n",
    "EDBatch = namedtuple('EDBatch', field_names=EDBatch_fields, defaults=[None] * len(EDBatch_fields))\n",
    "def ED_collate_fn(batch):\n",
    "    return EDBatch(\n",
    "        batch_doc_id=[instance[\"doc_id\"] for instance in batch],\n",
    "        batch_wnd_id=[instance[\"wnd_id\"] for instance in batch],\n",
    "        batch_tokens=[instance[\"tokens\"] for instance in batch], \n",
    "        batch_pieces=[instance[\"pieces\"] for instance in batch], \n",
    "        batch_token_lens=[instance[\"token_lens\"] for instance in batch], \n",
    "        batch_token_num=[instance[\"token_num\"] for instance in batch], \n",
    "        batch_text=[instance[\"text\"] for instance in batch], \n",
    "        batch_triggers=[instance[\"triggers\"] for instance in batch], \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train = DataLoader(internal_train_data, batch_size=3, \n",
    "                                                         shuffle=True, drop_last=False, collate_fn=ED_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EDBatch(batch_doc_id=['19354059_1', '18421192_3', '3365032_2'], batch_wnd_id=['19354059_1_1', '18421192_3_1', '3365032_2_1'], batch_tokens=[['The', 'most', 'common', 'complication', 'of', 'warfarin', 'use', 'is', 'adverse', 'bleeding', '.'], ['Type', '1', 'diabetes', 'mellitus', 'provoked', 'by', 'peginterferon', 'alpha', '-', '2b', 'plus', 'ribavirin', 'treatment', 'for', 'chronic', 'hepatitis', 'C.'], ['Polymyositis', 'after', 'propylthiouracil', 'treatment', 'for', 'hyperthyroidism', '.']], batch_pieces=[['ĠThe', 'Ġmost', 'Ġcommon', 'Ġcomplication', 'Ġof', 'Ġwar', 'far', 'in', 'Ġuse', 'Ġis', 'Ġadverse', 'Ġbleeding', 'Ġ.'], ['ĠType', 'Ġ1', 'Ġdiabetes', 'Ġmell', 'itus', 'Ġprovoked', 'Ġby', 'Ġpe', 'gin', 'ter', 'fer', 'on', 'Ġalpha', 'Ġ-', 'Ġ2', 'b', 'Ġplus', 'Ġrib', 'av', 'irin', 'Ġtreatment', 'Ġfor', 'Ġchronic', 'Ġhepatitis', 'ĠC', '.'], ['ĠPoly', 'my', 'osit', 'is', 'Ġafter', 'Ġprop', 'yl', 'th', 'iour', 'ac', 'il', 'Ġtreatment', 'Ġfor', 'Ġhyper', 'thy', 'roid', 'ism', 'Ġ.']], batch_token_lens=[[1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1], [1, 1, 1, 2, 1, 1, 5, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2], [4, 1, 6, 1, 1, 4, 1]], batch_token_num=[11, 17, 7], batch_text=['The most common complication of warfarin use is adverse bleeding .', 'Type 1 diabetes mellitus provoked by peginterferon alpha - 2b plus ribavirin treatment for chronic hepatitis C.', 'Polymyositis after propylthiouracil treatment for hyperthyroidism .'], batch_triggers=[[(6, 7, 'Adverse_event', 'use')], [(4, 5, 'Adverse_event', 'provoked')], [(1, 2, 'Adverse_event', 'after')]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger_seqlabels(triggers, token_num, specify_trigger=None):\n",
    "    labels = ['O'] * token_num\n",
    "    count = 0\n",
    "    for trigger in triggers:\n",
    "        start, end = trigger[0], trigger[1]\n",
    "        if end > token_num:\n",
    "            continue\n",
    "        trigger_type = trigger[2]\n",
    "\n",
    "        if specify_trigger is not None:\n",
    "            if trigger_type != specify_trigger:\n",
    "                continue\n",
    "\n",
    "        if any([labels[i] != 'O' for i in range(start, end)]):\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        labels[start] = 'B-{}'.format(trigger_type)\n",
    "        for i in range(start + 1, end):\n",
    "            labels[i] = 'I-{}'.format(trigger_type)\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 17, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.batch_token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_matrix_creat(tokens:list):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = Doc(nlp.vocab, words=tokens)\n",
    "    adj_matrix = np.eye(len(tokens))\n",
    "    nlp(doc)\n",
    "    for j in range(len(tokens)):\n",
    "        adj_matrix[j][doc[j].head.i] = 1\n",
    "        adj_matrix[doc[j].head.i][j] = 1\n",
    "    return adj_matrix\n",
    "\n",
    "def Zeropad(adj_matrix,pad_length):\n",
    "    adj_pad = np.pad(adj_matrix,(0,pad_length),'constant')\n",
    "    return adj_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import torch\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def process_data(batch):\n",
    "        enc_idxs = []\n",
    "        enc_attn = []\n",
    "        trigger_seqidxs = []\n",
    "        token_lens = []\n",
    "        token_nums = []\n",
    "        max_token_num = max(batch.batch_token_num)\n",
    "        adjs = []\n",
    "        \n",
    "        for tokens, pieces, triggers, token_len, token_num in zip(batch.batch_tokens, batch.batch_pieces, batch.batch_triggers, \n",
    "                                                                      batch.batch_token_lens, batch.batch_token_num):\n",
    "            adj = adj_matrix_creat(tokens)\n",
    "            piece_id = tokenizer.convert_tokens_to_ids(pieces)\n",
    "            enc_idx = [tokenizer.convert_tokens_to_ids(tokenizer.bos_token)] + piece_id + [tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "            \n",
    "            adjs.append(Zeropad(adj,(max_token_num - token_num)))\n",
    "            enc_idxs.append(enc_idx)\n",
    "            enc_attn.append([1]*len(enc_idx))  \n",
    "            \n",
    "            trigger_seq = get_trigger_seqlabels(triggers, len(tokens))\n",
    "            \n",
    "            token_lens.append(token_len)\n",
    "            token_nums.append(token_num)\n",
    "            if config.use_crf:\n",
    "                trigger_seqidxs.append([label_stoi[\"trigger\"][s] for s in trigger_seq] + [0] * (max_token_num-len(tokens)))\n",
    "            else:\n",
    "                trigger_seqidxs.append([label_stoi[\"trigger\"][s] for s in trigger_seq] + [-100] * (max_token_num-len(tokens)))\n",
    "        max_len = max([len(enc_idx) for enc_idx in enc_idxs])\n",
    "        adjs = torch.LongTensor(adjs)\n",
    "        enc_idxs = torch.LongTensor([enc_idx + [tokenizer.convert_tokens_to_ids(tokenizer.pad_token)]*(max_len-len(enc_idx)) for enc_idx in enc_idxs])\n",
    "        enc_attn = torch.LongTensor([enc_att + [0]*(max_len-len(enc_att)) for enc_att in enc_attn])\n",
    "        trigger_seqidxs = torch.LongTensor(trigger_seqidxs)\n",
    "        return enc_idxs, enc_attn, trigger_seqidxs, token_lens, torch.LongTensor(token_nums),adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m enc_idxs, enc_attn, trigger_seqidxs, token_lens, token_nums,adjs \u001b[38;5;241m=\u001b[39m process_data(batch\u001b[38;5;241m=\u001b[39mbatch)\n",
      "Cell \u001b[1;32mIn[40], line 13\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m adjs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens, pieces, triggers, token_len, token_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch\u001b[38;5;241m.\u001b[39mbatch_tokens, batch\u001b[38;5;241m.\u001b[39mbatch_pieces, batch\u001b[38;5;241m.\u001b[39mbatch_triggers, \n\u001b[0;32m     12\u001b[0m                                                               batch\u001b[38;5;241m.\u001b[39mbatch_token_lens, batch\u001b[38;5;241m.\u001b[39mbatch_token_num):\n\u001b[1;32m---> 13\u001b[0m     adj \u001b[38;5;241m=\u001b[39m adj_matrix_creat(tokens)\n\u001b[0;32m     14\u001b[0m     piece_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(pieces)\n\u001b[0;32m     15\u001b[0m     enc_idx \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokenizer\u001b[38;5;241m.\u001b[39mbos_token)] \u001b[38;5;241m+\u001b[39m piece_id \u001b[38;5;241m+\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokenizer\u001b[38;5;241m.\u001b[39meos_token)]\n",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m, in \u001b[0;36madj_matrix_creat\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madj_matrix_creat\u001b[39m(tokens:\u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     doc \u001b[38;5;241m=\u001b[39m Doc(nlp\u001b[38;5;241m.\u001b[39mvocab, words\u001b[38;5;241m=\u001b[39mtokens)\n\u001b[0;32m      4\u001b[0m     adj_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mlen\u001b[39m(tokens))\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "enc_idxs, enc_attn, trigger_seqidxs, token_lens, token_nums,adjs = process_data(batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lens_to_idxs(token_lens):\n",
    "        \"\"\"Map token lengths to a word piece index matrix (for torch.gather) and a\n",
    "        mask tensor.\n",
    "        For example (only show a sequence instead of a batch):\n",
    "        token lengths: [1,1,1,3,1]\n",
    "        =>\n",
    "        indices: [[0,0,0], [1,0,0], [2,0,0], [3,4,5], [6,0,0]]\n",
    "        masks: [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0],\n",
    "                [0.33, 0.33, 0.33], [1.0, 0.0, 0.0]]\n",
    "        Next, we use torch.gather() to select vectors of word pieces for each token,\n",
    "        and average them as follows (incomplete code):\n",
    "        outputs = torch.gather(bert_outputs, 1, indices) * masks\n",
    "        outputs = bert_outputs.view(batch_size, seq_len, -1, self.bert_dim)\n",
    "        outputs = bert_outputs.sum(2)\n",
    "        :param token_lens (list): token lengths.\n",
    "        :return: a index matrix and a mask tensor.\n",
    "        \"\"\"\n",
    "        max_token_num = max([len(x) for x in token_lens])  # 最大token数\n",
    "        max_token_len = max([max(x) for x in token_lens])  # token中最大的\n",
    "        idxs, masks = [], []\n",
    "        for seq_token_lens in token_lens:\n",
    "            seq_idxs, seq_masks = [], []\n",
    "            offset = 0\n",
    "            for token_len in seq_token_lens:\n",
    "                # max_token_len为一组\n",
    "                seq_idxs.extend([i + offset for i in range(token_len)]\n",
    "                                + [-1] * (max_token_len - token_len))\n",
    "                seq_masks.extend([1.0 / token_len] * token_len\n",
    "                                 + [0.0] * (max_token_len - token_len))\n",
    "                offset += token_len\n",
    "            #  补全 \n",
    "            seq_idxs.extend([-1] * max_token_len * (max_token_num - len(seq_token_lens)))\n",
    "            seq_masks.extend([0.0] * max_token_len * (max_token_num - len(seq_token_lens)))\n",
    "            idxs.append(seq_idxs)\n",
    "            masks.append(seq_masks)\n",
    "        return idxs, masks, max_token_num, max_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001B4F03511D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001B4F03B7B50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "base_model = RobertaModel.from_pretrained(config.pretrained_model_name, \n",
    "                                                           cache_dir=config.cache_dir, \n",
    "                                                           output_hidden_states=True)\n",
    "base_config = RobertaConfig.from_pretrained(config.pretrained_model_name, \n",
    "                                                             cache_dir=config.cache_dir)\n",
    "base_model_dim = base_config.hidden_size\n",
    "print(base_model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode( piece_idxs, attention_masks, token_lens):\n",
    "    \"\"\"Encode input sequences with BERT\n",
    "    :param piece_idxs (LongTensor): word pieces indices\n",
    "    :param attention_masks (FloatTensor): attention mask\n",
    "    :param token_lens (list): token lengths\n",
    "    \"\"\"\n",
    "    batch_size, _ = piece_idxs.size()\n",
    "    all_base_model_outputs = base_model(piece_idxs, attention_mask=attention_masks)\n",
    "    base_model_outputs = all_base_model_outputs[0] # (bs , seq_len , hidden_dim)\n",
    "    if config.multi_piece_strategy == 'first':\n",
    "        # select the first piece for multi-piece words\n",
    "        offsets = token_lens_to_offsets(token_lens)\n",
    "        offsets = piece_idxs.new(offsets) # batch x max_token_num\n",
    "        # + 1 because the first vector is for [CLS]\n",
    "        offsets = offsets.unsqueeze(-1).expand(batch_size, -1, bert_dim) + 1\n",
    "        base_model_outputs = torch.gather(base_model_outputs, 1, offsets)\n",
    "    elif config.multi_piece_strategy == 'average':\n",
    "        # average all pieces for multi-piece words\n",
    "        idxs, masks, token_num, token_len = token_lens_to_idxs(token_lens)   # idx,masks:(bs , max_token_num * max_token_len)\n",
    "        \n",
    "        idxs = piece_idxs.new(idxs).unsqueeze(-1).expand(batch_size, -1, base_model_dim) + 1 # idx:(bs , max_token_num * max_token_len, base_model_dim)\n",
    "\n",
    "        masks = base_model_outputs.new(masks).unsqueeze(-1) # masks:(bs , max_token_num * max_token_len, 1)\n",
    " \n",
    "        base_model_outputs = torch.gather(base_model_outputs, 1, idxs) * masks # (bs, max_token_num * max_token_len,base_model_dim)\n",
    "\n",
    "        base_model_outputs = base_model_outputs.view(batch_size, token_num, token_len, base_model_dim)\n",
    "        base_model_outputs = base_model_outputs.sum(2)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown multi-piece token handling strategy: {config.multi_piece_strategy}')\n",
    "    superbase_model_dropout = nn.Dropout(p=config.base_model_dropout)\n",
    "    base_model_outputs = superbase_model_dropout(base_model_outputs)\n",
    "    return base_model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "base_model_outputs = encode(enc_idxs, enc_attn, token_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "print(base_model_outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linears(nn.Module):\n",
    "    \"\"\"Multiple linear layers with Dropout.\"\"\"\n",
    "    def __init__(self, dimensions, activation='relu', dropout_prob=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        assert len(dimensions) > 1\n",
    "        self.layers = nn.ModuleList([nn.Linear(dimensions[i], dimensions[i + 1], bias=bias)\n",
    "                                     for i in range(len(dimensions) - 1)])\n",
    "        self.activation = getattr(torch, activation)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i > 0:\n",
    "                inputs = self.activation(inputs)\n",
    "                inputs = self.dropout(inputs)\n",
    "            inputs = layer(inputs)\n",
    "            outputs.append(inputs)\n",
    "        return outputs[-1]\n",
    "feature_dim = base_model_dim\n",
    "trigger_label_ffn = Linears([feature_dim, config.linear_hidden_num, len(label_stoi[\"trigger\"])],\n",
    "                                      dropout_prob=config.linear_dropout, \n",
    "                                      bias=config.linear_bias, \n",
    "                                      activation=config.linear_activation)\n",
    "entity_label_scores = trigger_label_ffn(base_model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 35, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_label_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(lens, max_len=None):\n",
    "    \"\"\"Generate a sequence mask tensor from sequence lengths, used by CRF.\"\"\"\n",
    "    batch_size = lens.size(0)\n",
    "    if max_len is None:\n",
    "        max_len = lens.max().item()\n",
    "    ranges = torch.arange(0, max_len, device=lens.device).long()\n",
    "    ranges = ranges.unsqueeze(0).expand(batch_size, max_len)\n",
    "    lens_exp = lens.unsqueeze(1).expand_as(ranges)\n",
    "    mask = ranges < lens_exp\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 29, 35])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sequence_mask(token_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trigger': {'O': 0,\n",
       "  'B-Adverse_event': 1,\n",
       "  'I-Adverse_event': 2,\n",
       "  'B-Potential_therapeutic_event': 3,\n",
       "  'I-Potential_therapeutic_event': 4}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "num_tags = len(label_stoi['trigger'])  # number of tags is 5\n",
    "model = CRF(num_tags,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigger_seqidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-56.9406, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissions = entity_label_scores\n",
    "tags = trigger_seqidxs\n",
    "model(emissions, tags,mask = mask,reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_paths_to_spans( paths, token_nums, vocab):\n",
    "        \"\"\"\n",
    "        Convert predicted tag paths to a list of spans (entity mentions or event\n",
    "        triggers).\n",
    "        :param paths: predicted tag paths.\n",
    "        :return (list): a list (batch) of lists (sequence) of spans.\n",
    "        \"\"\"\n",
    "        batch_mentions = []\n",
    "        itos = {i: s for s, i in vocab.items()}\n",
    "        for i, path in enumerate(paths):\n",
    "            mentions = []\n",
    "            cur_mention = None\n",
    "            path = path[:token_nums[i].item()]\n",
    "            for j, tag in enumerate(path):\n",
    "                tag = itos[tag]\n",
    "                if tag == 'O':\n",
    "                    prefix = tag = 'O'\n",
    "                else:\n",
    "                    prefix, tag = tag.split('-', 1)\n",
    "                if prefix == 'B':\n",
    "                    if cur_mention:\n",
    "                        mentions.append(cur_mention)\n",
    "                    cur_mention = [j, j + 1, tag]\n",
    "                elif prefix == 'I':\n",
    "                    if cur_mention is None:\n",
    "                        # treat it as B-*\n",
    "                        cur_mention = [j, j + 1, tag]\n",
    "                    elif cur_mention[-1] == tag:\n",
    "                        cur_mention[1] = j + 1\n",
    "                    else:\n",
    "                        # treat it as B-*\n",
    "                        mentions.append(cur_mention)\n",
    "                        cur_mention = [j, j + 1, tag]\n",
    "                else:\n",
    "                    if cur_mention:\n",
    "                        mentions.append(cur_mention)\n",
    "                    cur_mention = None\n",
    "            if cur_mention:\n",
    "                mentions.append(cur_mention)\n",
    "            batch_mentions.append(mentions)\n",
    "            \n",
    "        return batch_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 29, 35])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-Adverse_event': 1,\n",
       " 'I-Adverse_event': 2,\n",
       " 'B-Potential_therapeutic_event': 3,\n",
       " 'I-Potential_therapeutic_event': 4}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_stoi[\"trigger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  2],\n",
       " [3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4],\n",
       " [2,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  4]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(emissions,mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1, 'Potential_therapeutic_event'],\n",
       "  [1, 2, 'Adverse_event'],\n",
       "  [2, 3, 'Potential_therapeutic_event'],\n",
       "  [4, 5, 'Potential_therapeutic_event'],\n",
       "  [5, 6, 'Potential_therapeutic_event'],\n",
       "  [6, 7, 'Adverse_event'],\n",
       "  [7, 8, 'Potential_therapeutic_event'],\n",
       "  [8, 11, 'Potential_therapeutic_event'],\n",
       "  [11, 13, 'Potential_therapeutic_event'],\n",
       "  [13, 14, 'Potential_therapeutic_event'],\n",
       "  [14, 16, 'Potential_therapeutic_event'],\n",
       "  [17, 18, 'Adverse_event'],\n",
       "  [18, 20, 'Potential_therapeutic_event'],\n",
       "  [20, 22, 'Potential_therapeutic_event'],\n",
       "  [23, 24, 'Potential_therapeutic_event'],\n",
       "  [24, 26, 'Potential_therapeutic_event'],\n",
       "  [26, 27, 'Potential_therapeutic_event'],\n",
       "  [27, 28, 'Adverse_event'],\n",
       "  [28, 29, 'Potential_therapeutic_event'],\n",
       "  [29, 31, 'Potential_therapeutic_event'],\n",
       "  [31, 32, 'Adverse_event']],\n",
       " [[0, 1, 'Potential_therapeutic_event'],\n",
       "  [1, 3, 'Potential_therapeutic_event'],\n",
       "  [4, 6, 'Potential_therapeutic_event'],\n",
       "  [6, 7, 'Potential_therapeutic_event'],\n",
       "  [7, 8, 'Potential_therapeutic_event'],\n",
       "  [8, 9, 'Adverse_event'],\n",
       "  [9, 10, 'Potential_therapeutic_event'],\n",
       "  [11, 12, 'Potential_therapeutic_event'],\n",
       "  [12, 13, 'Adverse_event'],\n",
       "  [13, 14, 'Potential_therapeutic_event'],\n",
       "  [14, 15, 'Potential_therapeutic_event'],\n",
       "  [15, 16, 'Adverse_event'],\n",
       "  [16, 17, 'Potential_therapeutic_event'],\n",
       "  [17, 18, 'Adverse_event'],\n",
       "  [18, 19, 'Potential_therapeutic_event'],\n",
       "  [20, 21, 'Adverse_event'],\n",
       "  [21, 22, 'Potential_therapeutic_event'],\n",
       "  [22, 23, 'Adverse_event'],\n",
       "  [23, 24, 'Potential_therapeutic_event'],\n",
       "  [24, 25, 'Adverse_event'],\n",
       "  [25, 26, 'Adverse_event'],\n",
       "  [26, 27, 'Potential_therapeutic_event'],\n",
       "  [27, 28, 'Adverse_event'],\n",
       "  [28, 29, 'Potential_therapeutic_event']],\n",
       " [[0, 1, 'Adverse_event'],\n",
       "  [1, 2, 'Potential_therapeutic_event'],\n",
       "  [2, 3, 'Adverse_event'],\n",
       "  [3, 4, 'Potential_therapeutic_event'],\n",
       "  [4, 5, 'Adverse_event'],\n",
       "  [5, 6, 'Potential_therapeutic_event'],\n",
       "  [7, 8, 'Potential_therapeutic_event'],\n",
       "  [9, 10, 'Potential_therapeutic_event'],\n",
       "  [10, 11, 'Potential_therapeutic_event'],\n",
       "  [11, 12, 'Potential_therapeutic_event'],\n",
       "  [12, 13, 'Adverse_event'],\n",
       "  [13, 15, 'Potential_therapeutic_event'],\n",
       "  [15, 16, 'Potential_therapeutic_event'],\n",
       "  [16, 17, 'Potential_therapeutic_event'],\n",
       "  [17, 18, 'Adverse_event'],\n",
       "  [18, 19, 'Potential_therapeutic_event'],\n",
       "  [19, 21, 'Adverse_event'],\n",
       "  [21, 22, 'Potential_therapeutic_event'],\n",
       "  [23, 24, 'Adverse_event'],\n",
       "  [24, 25, 'Potential_therapeutic_event'],\n",
       "  [26, 27, 'Adverse_event'],\n",
       "  [27, 29, 'Potential_therapeutic_event'],\n",
       "  [30, 31, 'Adverse_event'],\n",
       "  [31, 32, 'Potential_therapeutic_event'],\n",
       "  [32, 33, 'Adverse_event'],\n",
       "  [33, 35, 'Potential_therapeutic_event']]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_paths_to_spans(model.decode(emissions,mask = mask), \n",
    "                    token_nums, \n",
    "                    label_stoi[\"trigger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(tensor, dim=0, keepdim: bool = False):\n",
    "    \"\"\"LogSumExp operation used by CRF.\"\"\"\n",
    "    m, _ = tensor.max(dim, keepdim=keepdim)\n",
    "    if keepdim:\n",
    "        stable_vec = tensor - m\n",
    "    else:\n",
    "        stable_vec = tensor - m.unsqueeze(dim)\n",
    "    return m + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()\n",
    "def sequence_mask(lens, max_len=None):\n",
    "    \"\"\"Generate a sequence mask tensor from sequence lengths, used by CRF.\"\"\"\n",
    "    batch_size = lens.size(0)\n",
    "    if max_len is None:\n",
    "        max_len = lens.max().item()\n",
    "    ranges = torch.arange(0, max_len, device=lens.device).long()\n",
    "    ranges = ranges.unsqueeze(0).expand(batch_size, max_len)\n",
    "    lens_exp = lens.unsqueeze(1).expand_as(ranges)\n",
    "    mask = ranges < lens_exp\n",
    "    return mask\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, label_vocab, bioes=False):\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        self.label_vocab = label_vocab\n",
    "        self.label_size = len(label_vocab) + 2\n",
    "        self.bioes = bioes\n",
    "\n",
    "        self.start = self.label_size - 2\n",
    "        self.end = self.label_size - 1\n",
    "        transition = torch.randn(self.label_size, self.label_size)\n",
    "        self.transition = nn.Parameter(transition)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.transition.data[:, self.end] = -100.0\n",
    "        self.transition.data[self.start, :] = -100.0\n",
    "\n",
    "        for label, label_idx in self.label_vocab.items():\n",
    "            if label.startswith('I-') or label.startswith('E-'):\n",
    "                self.transition.data[label_idx, self.start] = -100.0\n",
    "            if label.startswith('B-') or label.startswith('I-'):\n",
    "                self.transition.data[self.end, label_idx] = -100.0\n",
    "\n",
    "        for label_from, label_from_idx in self.label_vocab.items():\n",
    "            if label_from == 'O':\n",
    "                label_from_prefix, label_from_type = 'O', 'O'\n",
    "            else:\n",
    "                label_from_prefix, label_from_type = label_from.split('-', 1)\n",
    "\n",
    "            for label_to, label_to_idx in self.label_vocab.items():\n",
    "                if label_to == 'O':\n",
    "                    label_to_prefix, label_to_type = 'O', 'O'\n",
    "                else:\n",
    "                    label_to_prefix, label_to_type = label_to.split('-', 1)\n",
    "\n",
    "                if self.bioes:\n",
    "                    is_allowed = any(\n",
    "                        [\n",
    "                            label_from_prefix in ['O', 'E', 'S']\n",
    "                            and label_to_prefix in ['O', 'B', 'S'],\n",
    "\n",
    "                            label_from_prefix in ['B', 'I']\n",
    "                            and label_to_prefix in ['I', 'E']\n",
    "                            and label_from_type == label_to_type\n",
    "                        ]\n",
    "                    )\n",
    "                else:\n",
    "                    is_allowed = any(\n",
    "                        [\n",
    "                            label_to_prefix in ['B', 'O'],\n",
    "\n",
    "                            label_from_prefix in ['B', 'I']\n",
    "                            and label_to_prefix == 'I'\n",
    "                            and label_from_type == label_to_type\n",
    "                        ]\n",
    "                    )\n",
    "                if not is_allowed:\n",
    "                    self.transition.data[\n",
    "                        label_to_idx, label_from_idx] = -100.0\n",
    "\n",
    "    def pad_logits(self, logits):\n",
    "        \"\"\"Pad the linear layer output with <SOS> and <EOS> scores.\n",
    "        :param logits: Linear layer output (no non-linear function).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = logits.size()\n",
    "        pads = logits.new_full((batch_size, seq_len, 2), -100.0,\n",
    "                               requires_grad=False)\n",
    "        logits = torch.cat([logits, pads], dim=2)\n",
    "        return logits\n",
    "    \n",
    "    # 计算转移分数\n",
    "    def calc_binary_score(self, labels, lens):\n",
    "        batch_size, seq_len = labels.size()\n",
    "\n",
    "        # A tensor of size batch_size * (seq_len + 2)\n",
    "        labels_ext = labels.new_empty((batch_size, seq_len + 2))\n",
    "        labels_ext[:, 0] = self.start\n",
    "        labels_ext[:, 1:-1] = labels\n",
    "        mask = sequence_mask(lens + 1, max_len=(seq_len + 2)).long()\n",
    "        pad_stop = labels.new_full((1,), self.end, requires_grad=False)\n",
    "        pad_stop = pad_stop.unsqueeze(-1).expand(batch_size, seq_len + 2)\n",
    "        labels_ext = (1 - mask) * pad_stop + mask * labels_ext\n",
    "        labels = labels_ext\n",
    "\n",
    "        trn = self.transition\n",
    "        trn_exp = trn.unsqueeze(0).expand(batch_size, self.label_size,\n",
    "                                          self.label_size)\n",
    "        lbl_r = labels[:, 1:]\n",
    "        lbl_rexp = lbl_r.unsqueeze(-1).expand(*lbl_r.size(), self.label_size)\n",
    "        # score of jumping to a tag\n",
    "        trn_row = torch.gather(trn_exp, 1, lbl_rexp)\n",
    "\n",
    "        lbl_lexp = labels[:, :-1].unsqueeze(-1)\n",
    "        trn_scr = torch.gather(trn_row, 2, lbl_lexp)\n",
    "        trn_scr = trn_scr.squeeze(-1)\n",
    "\n",
    "        mask = sequence_mask(lens + 1).float()\n",
    "        trn_scr = trn_scr * mask\n",
    "        score = trn_scr\n",
    "\n",
    "        return score\n",
    "    \n",
    "    # 计算状态分数\n",
    "    def calc_unary_score(self, logits, labels, lens):\n",
    "        \"\"\"Checked\"\"\"\n",
    "        labels_exp = labels.unsqueeze(-1)\n",
    "        scores = torch.gather(logits, 2, labels_exp).squeeze(-1)\n",
    "        mask = sequence_mask(lens).float()\n",
    "        scores = scores * mask\n",
    "        return scores\n",
    "\n",
    "    def calc_gold_score(self, logits, labels, lens):\n",
    "        \"\"\"Checked\"\"\"\n",
    "        unary_score = self.calc_unary_score(logits, labels, lens).sum(\n",
    "            1).squeeze(-1)\n",
    "        binary_score = self.calc_binary_score(labels, lens).sum(1).squeeze(-1)\n",
    "        return unary_score + binary_score\n",
    "\n",
    "    def calc_norm_score(self, logits, lens):\n",
    "        batch_size, _, _ = logits.size()\n",
    "        alpha = logits.new_full((batch_size, self.label_size), -100.0)\n",
    "        alpha[:, self.start] = 0\n",
    "        lens_ = lens.clone()\n",
    "\n",
    "        logits_t = logits.transpose(1, 0)\n",
    "        for logit in logits_t:\n",
    "            logit_exp = logit.unsqueeze(-1).expand(batch_size,\n",
    "                                                   self.label_size,\n",
    "                                                   self.label_size)\n",
    "            alpha_exp = alpha.unsqueeze(1).expand(batch_size,\n",
    "                                                  self.label_size,\n",
    "                                                  self.label_size)\n",
    "            trans_exp = self.transition.unsqueeze(0).expand_as(alpha_exp)\n",
    "            mat = logit_exp + alpha_exp + trans_exp\n",
    "            alpha_nxt = log_sum_exp(mat, 2).squeeze(-1)\n",
    "\n",
    "            mask = (lens_ > 0).float().unsqueeze(-1).expand_as(alpha)\n",
    "            alpha = mask * alpha_nxt + (1 - mask) * alpha\n",
    "            lens_ = lens_ - 1\n",
    "\n",
    "        alpha = alpha + self.transition[self.end].unsqueeze(0).expand_as(alpha)\n",
    "        norm = log_sum_exp(alpha, 1).squeeze(-1)\n",
    "\n",
    "        return norm\n",
    "\n",
    "    def loglik(self, logits, labels, lens):\n",
    "        norm_score = self.calc_norm_score(logits, lens)\n",
    "        gold_score = self.calc_gold_score(logits, labels, lens)\n",
    "        return gold_score - norm_score\n",
    "\n",
    "    def viterbi_decode(self, logits, lens):\n",
    "        \"\"\"Borrowed from pytorch tutorial\n",
    "        Arguments:\n",
    "            logits: [batch_size, seq_len, n_labels] FloatTensor\n",
    "            lens: [batch_size] LongTensor\n",
    "        \"\"\"\n",
    "        batch_size, _, n_labels = logits.size()\n",
    "        vit = logits.new_full((batch_size, self.label_size), -100.0)\n",
    "        vit[:, self.start] = 0\n",
    "        c_lens = lens.clone()\n",
    "\n",
    "        logits_t = logits.transpose(1, 0)\n",
    "        pointers = []\n",
    "        for logit in logits_t:\n",
    "            vit_exp = vit.unsqueeze(1).expand(batch_size, n_labels, n_labels)\n",
    "            trn_exp = self.transition.unsqueeze(0).expand_as(vit_exp)\n",
    "            vit_trn_sum = vit_exp + trn_exp\n",
    "            vt_max, vt_argmax = vit_trn_sum.max(2)\n",
    "\n",
    "            vt_max = vt_max.squeeze(-1)\n",
    "            vit_nxt = vt_max + logit\n",
    "            pointers.append(vt_argmax.squeeze(-1).unsqueeze(0))\n",
    "\n",
    "            mask = (c_lens > 0).float().unsqueeze(-1).expand_as(vit_nxt)\n",
    "            vit = mask * vit_nxt + (1 - mask) * vit\n",
    "\n",
    "            mask = (c_lens == 1).float().unsqueeze(-1).expand_as(vit_nxt)\n",
    "            vit += mask * self.transition[self.end].unsqueeze(\n",
    "                0).expand_as(vit_nxt)\n",
    "\n",
    "            c_lens = c_lens - 1\n",
    "\n",
    "        pointers = torch.cat(pointers)\n",
    "        scores, idx = vit.max(1)\n",
    "        paths = [idx.unsqueeze(1)]\n",
    "        for argmax in reversed(pointers):\n",
    "            idx_exp = idx.unsqueeze(-1)\n",
    "            idx = torch.gather(argmax, 1, idx_exp)\n",
    "            idx = idx.squeeze(-1)\n",
    "\n",
    "            paths.insert(0, idx.unsqueeze(1))\n",
    "\n",
    "        paths = torch.cat(paths[1:], 1)\n",
    "        scores = scores.squeeze(-1)\n",
    "\n",
    "        return scores, paths\n",
    "\n",
    "    def calc_conf_score_(self, logits, labels):\n",
    "        batch_size, _, _ = logits.size()\n",
    "\n",
    "        logits_t = logits.transpose(1, 0)\n",
    "        scores = [[] for _ in range(batch_size)]\n",
    "        pre_labels = [self.start] * batch_size\n",
    "        for i, logit in enumerate(logits_t):\n",
    "            logit_exp = logit.unsqueeze(-1).expand(batch_size,\n",
    "                                                   self.label_size,\n",
    "                                                   self.label_size)\n",
    "            trans_exp = self.transition.unsqueeze(0).expand(batch_size,\n",
    "                                                            self.label_size,\n",
    "                                                            self.label_size)\n",
    "            score = logit_exp + trans_exp\n",
    "            score = score.view(-1, self.label_size * self.label_size) \\\n",
    "                .softmax(1)\n",
    "            for j in range(batch_size):\n",
    "                cur_label = labels[j][i]\n",
    "                cur_score = score[j][cur_label * self.label_size + pre_labels[j]]\n",
    "                scores[j].append(cur_score)\n",
    "                pre_labels[j] = cur_label\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_crf = CRF(label_stoi[\"trigger\"], bioes=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 35, 7])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_label_scores_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_paths_to_spans( paths, token_nums, vocab):\n",
    "        \"\"\"\n",
    "        Convert predicted tag paths to a list of spans (entity mentions or event\n",
    "        triggers).\n",
    "        :param paths: predicted tag paths.\n",
    "        :return (list): a list (batch) of lists (sequence) of spans.\n",
    "        \"\"\"\n",
    "        batch_mentions = []\n",
    "        itos = {i: s for s, i in vocab.items()}\n",
    "        for i, path in enumerate(paths):\n",
    "            mentions = []\n",
    "            cur_mention = None\n",
    "            path = path.tolist()[:token_nums[i].item()]\n",
    "            for j, tag in enumerate(path):\n",
    "                tag = itos[tag]\n",
    "                if tag == 'O':\n",
    "                    prefix = tag = 'O'\n",
    "                else:\n",
    "                    prefix, tag = tag.split('-', 1)\n",
    "                if prefix == 'B':\n",
    "                    if cur_mention:\n",
    "                        mentions.append(cur_mention)\n",
    "                    cur_mention = [j, j + 1, tag]\n",
    "                elif prefix == 'I':\n",
    "                    if cur_mention is None:\n",
    "                        # treat it as B-*\n",
    "                        cur_mention = [j, j + 1, tag]\n",
    "                    elif cur_mention[-1] == tag:\n",
    "                        cur_mention[1] = j + 1\n",
    "                    else:\n",
    "                        # treat it as B-*\n",
    "                        mentions.append(cur_mention)\n",
    "                        cur_mention = [j, j + 1, tag]\n",
    "                else:\n",
    "                    if cur_mention:\n",
    "                        mentions.append(cur_mention)\n",
    "                    cur_mention = None\n",
    "            if cur_mention:\n",
    "                mentions.append(cur_mention)\n",
    "            batch_mentions.append(mentions)\n",
    "            \n",
    "        return batch_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_scores_ = trigger_crf.pad_logits(entity_label_scores)\n",
    "_, entity_label_preds = trigger_crf.viterbi_decode(entity_label_scores_,\n",
    "                                                        token_nums)\n",
    "entities = tag_paths_to_spans(entity_label_preds, \n",
    "                                    token_nums, \n",
    "                                    label_stoi[\"trigger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-76.2858, -68.1855, -83.8794], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigger_crf.loglik(entity_label_scores_, \n",
    "                                                           trigger_seqidxs, \n",
    "                                                           token_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[    -0.112,      1.147,     -0.032,     -0.184,     -0.351,      1.541,\n",
       "           -100.000],\n",
       "        [    -0.821,      0.051,      0.238,     -0.445,     -0.877,     -0.638,\n",
       "           -100.000],\n",
       "        [  -100.000,      0.813,      0.533,   -100.000,   -100.000,   -100.000,\n",
       "           -100.000],\n",
       "        [    -0.096,      0.842,     -0.645,      0.967,     -1.543,     -0.322,\n",
       "           -100.000],\n",
       "        [  -100.000,   -100.000,   -100.000,      0.066,     -1.322,   -100.000,\n",
       "           -100.000],\n",
       "        [  -100.000,   -100.000,   -100.000,   -100.000,   -100.000,   -100.000,\n",
       "           -100.000],\n",
       "        [     0.985,   -100.000,   -100.000,   -100.000,   -100.000,      0.813,\n",
       "           -100.000]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=3,sci_mode=False)\n",
    "trigger_crf.transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_scores_ = trigger_crf.pad_logits(entity_label_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = token_nums\n",
    "labels = trigger_seqidxs\n",
    "logits = entity_label_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "        [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "        [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "        [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "        [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.new_full((1,), trigger_crf.end, requires_grad=False).unsqueeze(-1).expand(batch_size, seq_len + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 46])\n",
      "tensor([[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "        [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "        [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size, seq_len = labels.size()\n",
    "# A tensor of size batch_size * (seq_len + 2)\n",
    "labels_ext = labels.new_empty((batch_size, seq_len + 2))\n",
    "labels_ext[:, 0] = trigger_crf.start\n",
    "labels_ext[:, 1:-1] = labels\n",
    "mask = sequence_mask(lens + 1, max_len=(seq_len + 2)).long()\n",
    "\n",
    "pad_stop = labels.new_full((1,), trigger_crf.end, requires_grad=False)\n",
    "pad_stop = pad_stop.unsqueeze(-1).expand(batch_size, seq_len + 2)\n",
    "labels_ext = (1 - mask) * pad_stop + mask * labels_ext\n",
    "labels = labels_ext\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[    -0.112,      1.147,     -0.032,     -0.184,     -0.351,      1.541,\n",
       "           -100.000],\n",
       "        [    -0.821,      0.051,      0.238,     -0.445,     -0.877,     -0.638,\n",
       "           -100.000],\n",
       "        [  -100.000,      0.813,      0.533,   -100.000,   -100.000,   -100.000,\n",
       "           -100.000],\n",
       "        [    -0.096,      0.842,     -0.645,      0.967,     -1.543,     -0.322,\n",
       "           -100.000],\n",
       "        [  -100.000,   -100.000,   -100.000,      0.066,     -1.322,   -100.000,\n",
       "           -100.000],\n",
       "        [  -100.000,   -100.000,   -100.000,   -100.000,   -100.000,   -100.000,\n",
       "           -100.000],\n",
       "        [     0.985,   -100.000,   -100.000,   -100.000,   -100.000,      0.813,\n",
       "           -100.000]], requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigger_crf.transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = trigger_crf.transition\n",
    "trn_exp = trn.unsqueeze(0).expand(batch_size, trigger_crf.label_size,\n",
    "                                    trigger_crf.label_size)\n",
    "lbl_r = labels[:, 1:]\n",
    "lbl_rexp = lbl_r.unsqueeze(-1).expand(*lbl_r.size(), trigger_crf.label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of jumping to a tag\n",
    "trn_row = torch.gather(trn_exp, 1, lbl_rexp)\n",
    "\n",
    "lbl_lexp = labels[:, :-1].unsqueeze(-1)\n",
    "trn_scr = torch.gather(trn_row, 2, lbl_lexp)\n",
    "trn_scr = trn_scr.squeeze(-1)\n",
    "\n",
    "mask = sequence_mask(lens + 1).float()\n",
    "trn_scr = trn_scr * mask\n",
    "score = trn_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.541, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,\n",
       "         -0.112, -0.112, -0.112, -0.112, -0.821,  1.147, -0.112, -0.112, -0.112,\n",
       "         -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,\n",
       "         -0.112, -0.112, -0.112,  0.985, -0.000, -0.000, -0.000, -0.000, -0.000,\n",
       "         -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000],\n",
       "        [ 1.541, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,\n",
       "         -0.821,  1.147, -0.112, -0.112, -0.821,  1.147, -0.112, -0.112, -0.112,\n",
       "         -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,  0.985, -0.000,\n",
       "         -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000,\n",
       "         -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000, -0.000],\n",
       "        [ 1.541, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,\n",
       "         -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,\n",
       "         -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.821,  1.147,\n",
       "         -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,\n",
       "         -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112, -0.112,  0.985]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_lexp = labels[:, :-1].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6]],\n",
       "\n",
       "        [[5],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6],\n",
       "         [6]],\n",
       "\n",
       "        [[5],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_lexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   1.541],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.821],\n",
       "         [   1.147],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [   0.985],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000]],\n",
       "\n",
       "        [[   1.541],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.821],\n",
       "         [   1.147],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.821],\n",
       "         [   1.147],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [   0.985],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000],\n",
       "         [-100.000]],\n",
       "\n",
       "        [[   1.541],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.821],\n",
       "         [   1.147],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [  -0.112],\n",
       "         [   0.985]]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(trn_row, 2, lbl_lexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_score = trigger_crf.calc_binary_score(labels, lens).sum(1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.536, 4.560, 3.051], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unary_score + binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_loglik = trigger_crf.loglik(entity_label_scores_, \n",
    "                                                           trigger_seqidxs, \n",
    "                                                           token_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-78.1870, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_label_loglik.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
