{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from argparse import Namespace\n",
    "from models import *\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertConfig, RobertaConfig, XLMRobertaConfig, BertModel, RobertaModel, XLMRobertaModel,RobertaTokenizer\n",
    "from collections import namedtuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"EAE\";\n",
    "dataset = \"phee\";\n",
    "split = 1;\n",
    "model_type = \"CRFTagging\";\n",
    "pretrained_model_name = \"roberta-base\";\n",
    "pretrained_model_alias = {\n",
    "    \"roberta-base\": \"roberta-base\", \n",
    "};\n",
    "config_dict =  {\n",
    "        #// general config\n",
    "        \"task\": task, \n",
    "        \"dataset\": dataset,\n",
    "        \"model_type\": model_type, \n",
    "        \"gpu_device\": 0, \n",
    "        \"seed\": 0, \n",
    "        \"cache_dir\": \"./cache\", \n",
    "        \"output_dir\": \"./outputs/%s_%s_%s_split%s_%s\" % (model_type, task, dataset, split, pretrained_model_alias[pretrained_model_name]), \n",
    "        \"train_file\": \"../data/preprocessing/%s/split%s/train.json\" % (dataset, split),\n",
    "        \"dev_file\": \"../data/preprocessing/%s/split%s/dev.json\" % (dataset, split),\n",
    "        \"test_file\": \"../data/preprocessing/%s/split%s/test.json\" % (dataset, split),\n",
    "        \n",
    "        \n",
    "        #// model config\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"base_model_dropout\": 0.2,\n",
    "        \"use_crf\": True,\n",
    "        \"use_trigger_feature\": True,\n",
    "        \"use_type_feature\": True, \n",
    "        \"type_feature_num\": 100, \n",
    "        \"linear_hidden_num\": 150,\n",
    "        \"linear_dropout\": 0.2,\n",
    "        \"linear_bias\": True, \n",
    "        \"linear_activation\": \"relu\",\n",
    "        \"multi_piece_strategy\": \"average\", \n",
    "        \"max_length\": 200, \n",
    "        \n",
    "        # // train config\n",
    "        \"max_epoch\": 30,\n",
    "        \"warmup_epoch\": 5,\n",
    "        \"accumulate_step\": 1,\n",
    "        \"train_batch_size\": 6,\n",
    "        \"eval_batch_size\": 12,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"base_model_learning_rate\": 1e-05,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"base_model_weight_decay\": 1e-05,\n",
    "        \"grad_clipping\": 5.0,\n",
    "    }\n",
    "config = Namespace(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trainer\n",
    "VALID_TASKS = [\"E2E\", \"ED\", \"EAE\", \"EARL\"]\n",
    "\n",
    "TRAINER_MAP = {\n",
    "\n",
    "    (\"CRFTagging\", \"ED\"): CRFTaggingEDTrainer, \n",
    "    (\"CRFTagging\", \"EAE\"): CRFTaggingEAETrainer\n",
    "}\n",
    "trainer_class = TRAINER_MAP[(config.model_type, config.task)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3003 EAE instances (2 trigger types and 16 role types) from ../data/preprocessing/phee/split1/train.json\n",
      "Loaded 1011 EAE instances (2 trigger types and 16 role types) from ../data/preprocessing/phee/split1/dev.json\n",
      "Loaded 1005 EAE instances (2 trigger types and 16 role types) from ../data/preprocessing/phee/split1/test.json\n",
      "There are 2 trigger types and 16 role types in total\n"
     ]
    }
   ],
   "source": [
    "def load_EAE_data(file, add_extra_info_fn, config):\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as fp:\n",
    "        lines = fp.readlines()\n",
    "    data = [json.loads(line) for line in lines]\n",
    "    \n",
    "    instances = []\n",
    "    for dt in data:\n",
    "        \n",
    "        entities = dt['entity_mentions']\n",
    "\n",
    "        event_mentions = dt['event_mentions']\n",
    "        event_mentions.sort(key=lambda x: x['trigger']['start'])\n",
    "\n",
    "        entity_map = {entity['id']: entity for entity in entities}\n",
    "        for i, event_mention in enumerate(event_mentions):\n",
    "            # trigger = (start index, end index, event type, text span)\n",
    "            trigger = (event_mention['trigger']['start'], \n",
    "                       event_mention['trigger']['end'], \n",
    "                       event_mention['event_type'], \n",
    "                       event_mention['trigger']['text'])\n",
    "\n",
    "            arguments = []\n",
    "            for arg in event_mention['arguments']:\n",
    "                mapped_entity = entity_map[arg['entity_id']]\n",
    "                \n",
    "                # argument = (start index, end index, role type, text span)\n",
    "                argument = (mapped_entity['start'], mapped_entity['end'], arg['role'], arg['text'])\n",
    "                arguments.append(argument)\n",
    "\n",
    "            arguments.sort(key=lambda x: (x[0], x[1]))\n",
    "            \n",
    "            instance = {\"doc_id\": dt[\"doc_id\"], \n",
    "                        \"wnd_id\": dt[\"wnd_id\"], \n",
    "                        \"tokens\": dt[\"tokens\"], \n",
    "                        \"text\": dt[\"text\"], \n",
    "                        \"trigger\": trigger, \n",
    "                        \"arguments\": arguments, \n",
    "                       }\n",
    "\n",
    "            instances.append(instance)\n",
    "            \n",
    "    trigger_type_set = set()\n",
    "    for instance in instances:\n",
    "        trigger_type_set.add(instance['trigger'][2])\n",
    "\n",
    "    role_type_set = set()\n",
    "    for instance in instances:\n",
    "        for argument in instance[\"arguments\"]:\n",
    "            role_type_set.add(argument[2])\n",
    "                \n",
    "    type_set = {\"trigger\": trigger_type_set, \"role\": role_type_set}\n",
    "    \n",
    "    # approach-specific preprocessing\n",
    "    new_instances = add_extra_info_fn(instances, data, config)\n",
    "    assert len(new_instances) == len(instances)\n",
    "    \n",
    "    print('Loaded {} EAE instances ({} trigger types and {} role types) from {}'.format(\n",
    "        len(new_instances), len(trigger_type_set), len(role_type_set), file))\n",
    "    \n",
    "    return new_instances, type_set\n",
    "    \n",
    "    return new_instances, type_set\n",
    "if config.task == \"EAE\":\n",
    "        train_data, train_type_set = load_EAE_data(config.train_file, trainer_class.add_extra_info_fn, config)\n",
    "        dev_data, dev_type_set = load_EAE_data(config.dev_file, trainer_class.add_extra_info_fn, config)\n",
    "        test_data, test_type_set = load_EAE_data(config.test_file, trainer_class.add_extra_info_fn, config)\n",
    "        type_set = {\"trigger\": train_type_set[\"trigger\"] | dev_type_set[\"trigger\"] | test_type_set[\"trigger\"], \n",
    "                    \"role\": train_type_set[\"role\"] | dev_type_set[\"role\"] | test_type_set[\"role\"]}\n",
    "        print(\"There are {} trigger types and {} role types in total\".format(len(type_set[\"trigger\"]), len(type_set[\"role\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '10082597_3',\n",
       " 'wnd_id': '10082597_3_1',\n",
       " 'tokens': ['RESULTS',\n",
       "  ':',\n",
       "  'A',\n",
       "  '44',\n",
       "  '-',\n",
       "  'year',\n",
       "  '-',\n",
       "  'old',\n",
       "  'man',\n",
       "  'taking',\n",
       "  'naproxen',\n",
       "  'for',\n",
       "  'chronic',\n",
       "  'low',\n",
       "  'back',\n",
       "  'pain',\n",
       "  'and',\n",
       "  'a',\n",
       "  '20',\n",
       "  '-',\n",
       "  'year',\n",
       "  '-',\n",
       "  'old',\n",
       "  'woman',\n",
       "  'on',\n",
       "  'oxaprozin',\n",
       "  'for',\n",
       "  'rheumatoid',\n",
       "  'arthritis',\n",
       "  'presented',\n",
       "  'with',\n",
       "  'tense',\n",
       "  'bullae',\n",
       "  'and',\n",
       "  'cutaneous',\n",
       "  'fragility',\n",
       "  'on',\n",
       "  'the',\n",
       "  'face',\n",
       "  'and',\n",
       "  'the',\n",
       "  'back',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hands',\n",
       "  '.'],\n",
       " 'text': 'RESULTS : A 44 - year - old man taking naproxen for chronic low back pain and a 20 - year - old woman on oxaprozin for rheumatoid arthritis presented with tense bullae and cutaneous fragility on the face and the back of the hands .',\n",
       " 'trigger': (29, 30, 'Adverse_event', 'presented'),\n",
       " 'arguments': [(17, 24, 'Subject', 'a 20 - year - old woman'),\n",
       "  (18, 23, 'Subject_Age', '20 - year - old'),\n",
       "  (23, 24, 'Subject_Gender', 'woman'),\n",
       "  (25, 26, 'Treatment', 'oxaprozin'),\n",
       "  (25, 26, 'Treatment_Drug', 'oxaprozin'),\n",
       "  (27, 29, 'Treatment_Disorder', 'rheumatoid arthritis'),\n",
       "  (31,\n",
       "   45,\n",
       "   'Effect',\n",
       "   'tense bullae and cutaneous fragility on the face and the back of the hands')],\n",
       " 'extra_info': None}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '10082597_3',\n",
       " 'wnd_id': '10082597_3_1',\n",
       " 'tokens': ['RESULTS',\n",
       "  ':',\n",
       "  'A',\n",
       "  '44',\n",
       "  '-',\n",
       "  'year',\n",
       "  '-',\n",
       "  'old',\n",
       "  'man',\n",
       "  'taking',\n",
       "  'naproxen',\n",
       "  'for',\n",
       "  'chronic',\n",
       "  'low',\n",
       "  'back',\n",
       "  'pain',\n",
       "  'and',\n",
       "  'a',\n",
       "  '20',\n",
       "  '-',\n",
       "  'year',\n",
       "  '-',\n",
       "  'old',\n",
       "  'woman',\n",
       "  'on',\n",
       "  'oxaprozin',\n",
       "  'for',\n",
       "  'rheumatoid',\n",
       "  'arthritis',\n",
       "  'presented',\n",
       "  'with',\n",
       "  'tense',\n",
       "  'bullae',\n",
       "  'and',\n",
       "  'cutaneous',\n",
       "  'fragility',\n",
       "  'on',\n",
       "  'the',\n",
       "  'face',\n",
       "  'and',\n",
       "  'the',\n",
       "  'back',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hands',\n",
       "  '.'],\n",
       " 'text': 'RESULTS : A 44 - year - old man taking naproxen for chronic low back pain and a 20 - year - old woman on oxaprozin for rheumatoid arthritis presented with tense bullae and cutaneous fragility on the face and the back of the hands .',\n",
       " 'trigger': (29, 30, 'Adverse_event', 'presented'),\n",
       " 'arguments': [(2, 9, 'Subject', 'A 44 - year - old man'),\n",
       "  (3, 8, 'Subject_Age', '44 - year - old'),\n",
       "  (8, 9, 'Subject_Gender', 'man'),\n",
       "  (10, 11, 'Treatment', 'naproxen'),\n",
       "  (10, 11, 'Treatment_Drug', 'naproxen'),\n",
       "  (12, 16, 'Treatment_Disorder', 'chronic low back pain'),\n",
       "  (31,\n",
       "   45,\n",
       "   'Effect',\n",
       "   'tense bullae and cutaneous fragility on the face and the back of the hands')],\n",
       " 'extra_info': None}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer = trainer_class(config, type_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(config.pretrained_model_name, cache_dir=config.cache_dir, do_lower_case=False, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing overlapping arguments and over-length examples\n",
      "There are 3003/3003 EAE instances after removing overlapping arguments and over-length examples\n",
      "Removing overlapping arguments and over-length examples\n",
      "There are 1011/1011 EAE instances after removing overlapping arguments and over-length examples\n"
     ]
    }
   ],
   "source": [
    "def process_data( data):\n",
    "        assert tokenizer, \"Please load model and tokneizer before processing data!\"\n",
    "        \n",
    "        print(\"Removing overlapping arguments and over-length examples\")\n",
    "        \n",
    "        # greedily remove overlapping arguments\n",
    "        n_total = 0\n",
    "        new_data = []\n",
    "        for dt in data:\n",
    "            \n",
    "            n_total += 1\n",
    "            \n",
    "            if len(dt[\"tokens\"]) > config.max_length:\n",
    "                continue\n",
    "            \n",
    "            trigger = dt[\"trigger\"]\n",
    "            no_overlap_flag = np.ones((len(dt[\"tokens\"]), ), dtype=bool)\n",
    "            new_arguments = []\n",
    "            for argument in sorted(dt[\"arguments\"]):\n",
    "                start, end = argument[0], argument[1]\n",
    "                if np.all(no_overlap_flag[start:end]):\n",
    "                    new_arguments.append(argument)\n",
    "                    no_overlap_flag[start:end] = False\n",
    "            \n",
    "            pieces = [tokenizer.tokenize(t, is_split_into_words=True) for t in dt[\"tokens\"]]\n",
    "            token_lens = [len(p) for p in pieces] \n",
    "\n",
    "            new_dt = {\"doc_id\": dt[\"doc_id\"], \n",
    "                      \"wnd_id\": dt[\"wnd_id\"], \n",
    "                      \"tokens\": dt[\"tokens\"], \n",
    "                      \"pieces\": [p for w in pieces for p in w], \n",
    "                      \"token_lens\": token_lens, \n",
    "                      \"token_num\": len(dt[\"tokens\"]), \n",
    "                      \"text\": dt[\"text\"], \n",
    "                      \"trigger\": dt[\"trigger\"], \n",
    "                      \"arguments\": new_arguments\n",
    "                     }\n",
    "            \n",
    "            \n",
    "            new_data.append(new_dt)\n",
    "                \n",
    "        print(f\"There are {len(new_data)}/{n_total} EAE instances after removing overlapping arguments and over-length examples\")\n",
    "\n",
    "        return new_data\n",
    "internal_train_data = process_data(train_data)\n",
    "internal_dev_data = process_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '10048291_2',\n",
       " 'wnd_id': '10048291_2_1',\n",
       " 'tokens': ['Unaccountable',\n",
       "  'severe',\n",
       "  'hypercalcemia',\n",
       "  'in',\n",
       "  'a',\n",
       "  'patient',\n",
       "  'treated',\n",
       "  'for',\n",
       "  'hypoparathyroidism',\n",
       "  'with',\n",
       "  'dihydrotachysterol',\n",
       "  '.'],\n",
       " 'text': 'Unaccountable severe hypercalcemia in a patient treated for hypoparathyroidism with dihydrotachysterol .',\n",
       " 'trigger': (6, 7, 'Adverse_event', 'treated'),\n",
       " 'arguments': [(0, 3, 'Effect', 'Unaccountable severe hypercalcemia'),\n",
       "  (5, 6, 'Subject', 'patient'),\n",
       "  (8, 9, 'Treatment_Disorder', 'hypoparathyroidism'),\n",
       "  (10, 11, 'Treatment', 'dihydrotachysterol'),\n",
       "  (10, 11, 'Treatment_Drug', 'dihydrotachysterol')],\n",
       " 'extra_info': None}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EAEBatch_fields = ['batch_doc_id', 'batch_wnd_id', 'batch_tokens', 'batch_pieces', 'batch_token_lens', 'batch_token_num', 'batch_text', 'batch_trigger', 'batch_arguments']\n",
    "EAEBatch = namedtuple('EAEBatch', field_names=EAEBatch_fields, defaults=[None] * len(EAEBatch_fields))\n",
    "\n",
    "def EAE_collate_fn(batch):\n",
    "    return EAEBatch(\n",
    "        batch_doc_id=[instance[\"doc_id\"] for instance in batch],\n",
    "        batch_wnd_id=[instance[\"wnd_id\"] for instance in batch],\n",
    "        batch_tokens=[instance[\"tokens\"] for instance in batch], \n",
    "        batch_pieces=[instance[\"pieces\"] for instance in batch], \n",
    "        batch_token_lens=[instance[\"token_lens\"] for instance in batch], \n",
    "        batch_token_num=[instance[\"token_num\"] for instance in batch], \n",
    "        batch_text=[instance[\"text\"] for instance in batch], \n",
    "        batch_trigger=[instance[\"trigger\"] for instance in batch], \n",
    "        batch_arguments=[instance[\"arguments\"] for instance in batch], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train = DataLoader(internal_train_data, batch_size=3, \n",
    "                                                         shuffle=True, drop_last=False, collate_fn=EAE_collate_fn)\n",
    "batch = next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tagging_vocab():\n",
    "    prefix = ['B', 'I']\n",
    "    trigger_label_stoi = {'O': 0}\n",
    "    for t in sorted(type_set[\"trigger\"]):\n",
    "        for p in prefix:\n",
    "            trigger_label_stoi['{}-{}'.format(p, t)] = len(trigger_label_stoi)\n",
    "\n",
    "    role_label_stoi = {'O': 0}\n",
    "    for t in sorted(type_set[\"role\"]):\n",
    "        for p in prefix:\n",
    "            role_label_stoi['{}-{}'.format(p, t)] = len(role_label_stoi)\n",
    "    \n",
    "    label_stoi = {\"trigger\": trigger_label_stoi, \"role\": role_label_stoi}\n",
    "    \n",
    "    trigger_type_stoi = {t: i for i, t in enumerate(sorted(type_set[\"trigger\"]))}\n",
    "    role_type_stoi = {t: i for i, t in enumerate(sorted(type_set[\"role\"]))}\n",
    "    type_stoi = {\"trigger\": trigger_type_stoi, \"role\": role_type_stoi}\n",
    "    return label_stoi,type_stoi\n",
    "label_stoi,type_stoi = generate_tagging_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trigger': {'O': 0,\n",
       "  'B-Adverse_event': 1,\n",
       "  'I-Adverse_event': 2,\n",
       "  'B-Potential_therapeutic_event': 3,\n",
       "  'I-Potential_therapeutic_event': 4},\n",
       " 'role': {'O': 0,\n",
       "  'B-Combination_Drug': 1,\n",
       "  'I-Combination_Drug': 2,\n",
       "  'B-Effect': 3,\n",
       "  'I-Effect': 4,\n",
       "  'B-Subject': 5,\n",
       "  'I-Subject': 6,\n",
       "  'B-Subject_Age': 7,\n",
       "  'I-Subject_Age': 8,\n",
       "  'B-Subject_Disorder': 9,\n",
       "  'I-Subject_Disorder': 10,\n",
       "  'B-Subject_Gender': 11,\n",
       "  'I-Subject_Gender': 12,\n",
       "  'B-Subject_Population': 13,\n",
       "  'I-Subject_Population': 14,\n",
       "  'B-Subject_Race': 15,\n",
       "  'I-Subject_Race': 16,\n",
       "  'B-Treatment': 17,\n",
       "  'I-Treatment': 18,\n",
       "  'B-Treatment_Disorder': 19,\n",
       "  'I-Treatment_Disorder': 20,\n",
       "  'B-Treatment_Dosage': 21,\n",
       "  'I-Treatment_Dosage': 22,\n",
       "  'B-Treatment_Drug': 23,\n",
       "  'I-Treatment_Drug': 24,\n",
       "  'B-Treatment_Duration': 25,\n",
       "  'I-Treatment_Duration': 26,\n",
       "  'B-Treatment_Freq': 27,\n",
       "  'I-Treatment_Freq': 28,\n",
       "  'B-Treatment_Route': 29,\n",
       "  'I-Treatment_Route': 30,\n",
       "  'B-Treatment_Time_elapsed': 31,\n",
       "  'I-Treatment_Time_elapsed': 32}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_seqlabels(roles, token_num, specify_role=None):\n",
    "        labels = ['O'] * token_num\n",
    "        count = 0\n",
    "        for role in roles:\n",
    "            start, end = role[0], role[1]\n",
    "            if end > token_num:\n",
    "                continue\n",
    "            role_type = role[2]\n",
    "\n",
    "            if specify_role is not None:\n",
    "                if role_type != specify_role:\n",
    "                    continue\n",
    "\n",
    "            if any([labels[i] != 'O' for i in range(start, end)]):\n",
    "                count += 1\n",
    "                continue\n",
    "\n",
    "            labels[start] = 'B-{}'.format(role_type)\n",
    "            for i in range(start + 1, end):\n",
    "                labels[i] = 'I-{}'.format(role_type)\n",
    "                \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'B-Treatment',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'I-Effect',\n",
       " 'O']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_role_seqlabels(batch.batch_arguments[0],len(batch.batch_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.batch_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(batch):\n",
    "        enc_idxs = []\n",
    "        enc_attn = []\n",
    "        role_seqidxs = []\n",
    "        trigger_types = []\n",
    "        token_lens = []\n",
    "        token_nums = []\n",
    "        triggers = []\n",
    "        max_token_num = max(batch.batch_token_num)\n",
    "        \n",
    "        for tokens, pieces, trigger, arguments, token_len, token_num in zip(batch.batch_tokens, batch.batch_pieces, batch.batch_trigger, \n",
    "                                                                      batch.batch_arguments, batch.batch_token_lens, batch.batch_token_num):\n",
    "            \n",
    "            piece_id = tokenizer.convert_tokens_to_ids(pieces)\n",
    "            enc_idx = [tokenizer.convert_tokens_to_ids(tokenizer.bos_token)] + piece_id + [tokenizer.convert_tokens_to_ids(tokenizer.eos_token)]\n",
    "            \n",
    "            enc_idxs.append(enc_idx)\n",
    "            enc_attn.append([1]*len(enc_idx))  \n",
    "            \n",
    "            role_seq = get_role_seqlabels(arguments, len(tokens))\n",
    "            trigger_types.append(type_stoi[\"trigger\"][trigger[2]])\n",
    "            token_lens.append(token_len)\n",
    "            token_nums.append(token_num)\n",
    "            triggers.append(trigger)\n",
    "            if config.use_crf:\n",
    "                role_seqidxs.append([label_stoi[\"role\"][s] for s in role_seq] + [0] * (max_token_num-len(tokens)))\n",
    "            else:\n",
    "                role_seqidxs.append([label_stoi[\"role\"][s] for s in role_seq] + [-100] * (max_token_num-len(tokens)))\n",
    "        max_len = max([len(enc_idx) for enc_idx in enc_idxs])\n",
    "        enc_idxs = torch.LongTensor([enc_idx + [tokenizer.convert_tokens_to_ids(tokenizer.pad_token)]*(max_len-len(enc_idx)) for enc_idx in enc_idxs])\n",
    "        enc_attn = torch.LongTensor([enc_att + [0]*(max_len-len(enc_att)) for enc_att in enc_attn])\n",
    "        trigger_types = torch.LongTensor(trigger_types)\n",
    "        return enc_idxs, enc_attn, role_seqidxs, trigger_types, token_lens, torch.LongTensor(token_nums),triggers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_idxs, enc_attn, role_seqidxs, trigger_types, token_lens, token_nums, triggers = process_data(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 16, 'Adverse_event', 'induce'),\n",
       " (8, 9, 'Adverse_event', 'diagnosed'),\n",
       " (70, 71, 'Adverse_event', 'experiencing')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "base_model = RobertaModel.from_pretrained(config.pretrained_model_name, \n",
    "                                                           cache_dir=config.cache_dir, \n",
    "                                                           output_hidden_states=True)\n",
    "base_config = RobertaConfig.from_pretrained(config.pretrained_model_name, \n",
    "                                                             cache_dir=config.cache_dir)\n",
    "base_model_dim = base_config.hidden_size\n",
    "print(base_model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lens_to_idxs( token_lens):\n",
    "        \"\"\"Map token lengths to a word piece index matrix (for torch.gather) and a\n",
    "        mask tensor.\n",
    "        For example (only show a sequence instead of a batch):\n",
    "        token lengths: [1,1,1,3,1]\n",
    "        =>\n",
    "        indices: [[0,0,0], [1,0,0], [2,0,0], [3,4,5], [6,0,0]]\n",
    "        masks: [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0],\n",
    "                [0.33, 0.33, 0.33], [1.0, 0.0, 0.0]]\n",
    "        Next, we use torch.gather() to select vectors of word pieces for each token,\n",
    "        and average them as follows (incomplete code):\n",
    "        outputs = torch.gather(bert_outputs, 1, indices) * masks\n",
    "        outputs = bert_outputs.view(batch_size, seq_len, -1, self.bert_dim)\n",
    "        outputs = bert_outputs.sum(2)\n",
    "        :param token_lens (list): token lengths.\n",
    "        :return: a index matrix and a mask tensor.\n",
    "        \"\"\"\n",
    "        max_token_num = max([len(x) for x in token_lens])\n",
    "        max_token_len = max([max(x) for x in token_lens])\n",
    "        idxs, masks = [], []\n",
    "        for seq_token_lens in token_lens:\n",
    "            seq_idxs, seq_masks = [], []\n",
    "            offset = 0\n",
    "            for token_len in seq_token_lens:\n",
    "                seq_idxs.extend([i + offset for i in range(token_len)]\n",
    "                                + [-1] * (max_token_len - token_len))\n",
    "                seq_masks.extend([1.0 / token_len] * token_len\n",
    "                                 + [0.0] * (max_token_len - token_len))\n",
    "                offset += token_len\n",
    "            seq_idxs.extend([-1] * max_token_len * (max_token_num - len(seq_token_lens)))\n",
    "            seq_masks.extend([0.0] * max_token_len * (max_token_num - len(seq_token_lens)))\n",
    "            idxs.append(seq_idxs)\n",
    "            masks.append(seq_masks)\n",
    "        return idxs, masks, max_token_num, max_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_dropout = nn.Dropout(p=config.base_model_dropout)\n",
    "def encode(piece_idxs, attention_masks, token_lens):\n",
    "    \"\"\"Encode input sequences with BERT\n",
    "    :param piece_idxs (LongTensor): word pieces indices\n",
    "    :param attention_masks (FloatTensor): attention mask\n",
    "    :param token_lens (list): token lengths\n",
    "    \"\"\"\n",
    "    batch_size, _ = piece_idxs.size()\n",
    "    all_base_model_outputs = base_model(piece_idxs, attention_mask=attention_masks)\n",
    "    base_model_outputs = all_base_model_outputs[0]\n",
    "    if config.multi_piece_strategy == 'first':\n",
    "        # select the first piece for multi-piece words\n",
    "        offsets = token_lens_to_offsets(token_lens)\n",
    "        offsets = piece_idxs.new(offsets) # batch x max_token_num\n",
    "        # + 1 because the first vector is for [CLS]\n",
    "        offsets = offsets.unsqueeze(-1).expand(batch_size, -1, self.bert_dim) + 1\n",
    "        base_model_outputs = torch.gather(base_model_outputs, 1, offsets)\n",
    "    elif config.multi_piece_strategy == 'average':\n",
    "        # average all pieces for multi-piece words\n",
    "        idxs, masks, token_num, token_len = token_lens_to_idxs(token_lens)\n",
    "        idxs = piece_idxs.new(idxs).unsqueeze(-1).expand(batch_size, -1, base_model_dim) + 1\n",
    "        masks = base_model_outputs.new(masks).unsqueeze(-1)\n",
    "        base_model_outputs = torch.gather(base_base_model_outputs.size()model_outputs, 1, idxs) * masks\n",
    "        base_model_outputs = base_model_outputs.view(batch_size, token_num, token_len, base_model_dim)\n",
    "        base_model_outputs = base_model_outputs.sum(2)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown multi-piece token handling strategy: {config.multi_piece_strategy}')\n",
    "    base_model_outputs = base_model_dropout(base_model_outputs)\n",
    "    return base_model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "base_model_outputs = encode(enc_idxs, enc_attn, token_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 92, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigger_embedding(base_model_outputs, triggers):\n",
    "        masks = []\n",
    "        max_tokens = base_model_outputs.size(1)\n",
    "        for trigger in triggers:\n",
    "            seq_masks = [0] * max_tokens\n",
    "            for element in range(trigger[0], trigger[1]):\n",
    "                seq_masks[element] = 1\n",
    "            masks.append(seq_masks)\n",
    "        masks = base_model_outputs.new(masks)\n",
    "        average = ((base_model_outputs*masks.unsqueeze(-1))/((masks.sum(dim=1,keepdim=True)).unsqueeze(-1))).sum(1)\n",
    "\n",
    "        return average # batch x bert_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "trigger_vec = get_trigger_embedding(base_model_outputs, triggers)\n",
    "print(trigger_vec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 92, 768])\n"
     ]
    }
   ],
   "source": [
    "extend_tri_vec = trigger_vec.unsqueeze(1).repeat(1, base_model_outputs.size(1), 1)\n",
    "print(extend_tri_vec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = base_model_dim*2 if config.use_trigger_feature else base_model_dim\n",
    "if config.use_type_feature:\n",
    "        feature_dim += config.type_feature_num\n",
    "        type_feature_module = nn.Embedding(len(type_set[\"trigger\"]), config.type_feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
